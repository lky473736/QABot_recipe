"""
KcBERTì˜ ì‹¤ì œ ì°¨ì› 300ì— ë§ì¶˜ ëª¨ë¸ í›ˆë ¨
ëª¨ë“  ì„¤ì •ì„ 300ìœ¼ë¡œ í†µì¼
"""
import json
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup
from torch.optim import AdamW
from sklearn.model_selection import train_test_split
import numpy as np
from tqdm import tqdm
import sys
import os
from typing import List, Dict, Any
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from config import *

class RecipeQADataset(Dataset):
    """ë ˆì‹œí”¼ QA ë°ì´í„°ì…‹ - 300 ê¸¸ì´ë¡œ ì œí•œ"""
    
    def __init__(self, qa_data, tokenizer, max_length=300):  # 300ìœ¼ë¡œ ë³€ê²½
        self.qa_data = qa_data
        self.tokenizer = tokenizer
        self.max_length = max_length
        print(f"ğŸ“Š ë°ì´í„°ì…‹ ìƒì„±: {len(qa_data)}ê°œ QA, ìµœëŒ€ ê¸¸ì´: {max_length}")
    
    def __len__(self):
        return len(self.qa_data)
    
    def __getitem__(self, idx):
        qa = self.qa_data[idx]
        
        if isinstance(qa, dict):
            question = str(qa.get('question', ''))
            answer = str(qa.get('answer', ''))
        else:
            question = "ì§ˆë¬¸ ì—†ìŒ"
            answer = "ë‹µë³€ ì—†ìŒ"
        
        # ì§ˆë¬¸ë§Œ ì¸ì½”ë”© (300 ê¸¸ì´ë¡œ)
        encoding = self.tokenizer(
            question,
            add_special_tokens=True,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'question': question,
            'answer': answer
        }

class KcBERT300Model(nn.Module):
    """KcBERT 300 ì°¨ì›ì— ìµœì í™”ëœ ëª¨ë¸"""
    
    def __init__(self, model_name='beomi/kcbert-base'):
        super(KcBERT300Model, self).__init__()
        
        print("ğŸ”§ KcBERT 300 ì°¨ì› ëª¨ë¸ ë¡œë“œ ì¤‘...")
        
        # KcBERT ë¡œë“œ
        self.bert = AutoModel.from_pretrained(model_name)
        
        # ì‹¤ì œ ì°¨ì› í™•ì¸
        self.hidden_size = 300  # KcBERTì˜ ì‹¤ì œ ì°¨ì›ìœ¼ë¡œ ê³ ì •
        print(f"ğŸ“ ê³ ì • hidden_size: {self.hidden_size}")
        
        self.dropout = nn.Dropout(0.1)
        self.classifier = nn.Linear(self.hidden_size, 1)
        
        print("âœ… KcBERT 300 ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def forward(self, input_ids, attention_mask):
        # ì…ë ¥ ê¸¸ì´ë¥¼ 300ìœ¼ë¡œ ì œí•œ
        if input_ids.size(1) > 300:
            input_ids = input_ids[:, :300]
            attention_mask = attention_mask[:, :300]
        
        try:
            # BERT ìˆœì „íŒŒ
            outputs = self.bert(
                input_ids=input_ids,
                attention_mask=attention_mask,
                return_dict=True
            )
            
            # ì¶œë ¥ ì¶”ì¶œ ì‹œë„
            pooled_output = None
            
            # ë°©ë²• 1: pooler_output ì‚¬ìš©
            if hasattr(outputs, 'pooler_output') and outputs.pooler_output is not None:
                pooled_output = outputs.pooler_output
                print(f"âœ… pooler_output ì‚¬ìš©: {pooled_output.shape}")
            
            # ë°©ë²• 2: [CLS] í† í° ì‚¬ìš©
            if pooled_output is None:
                last_hidden_state = outputs.last_hidden_state
                pooled_output = last_hidden_state[:, 0, :]  # [CLS] í† í°
                print(f"âœ… [CLS] í† í° ì‚¬ìš©: {pooled_output.shape}")
            
            # ì°¨ì›ì„ 300ìœ¼ë¡œ ê°•ì œ ì¡°ì •
            if pooled_output.size(-1) != 300:
                if pooled_output.size(-1) > 300:
                    # ì˜ë¼ë‚´ê¸°
                    pooled_output = pooled_output[:, :300]
                    print(f"ğŸ”§ ì°¨ì› ì˜ë¼ëƒ„: -> {pooled_output.shape}")
                else:
                    # íŒ¨ë”©
                    batch_size = pooled_output.size(0)
                    padding_size = 300 - pooled_output.size(-1)
                    padding = torch.zeros(batch_size, padding_size, device=pooled_output.device)
                    pooled_output = torch.cat([pooled_output, padding], dim=-1)
                    print(f"ğŸ”§ ì°¨ì› íŒ¨ë”©: -> {pooled_output.shape}")
            
            # ë“œë¡­ì•„ì›ƒ ì ìš©
            pooled_output = self.dropout(pooled_output)
            
            # ë¶„ë¥˜ ì ìˆ˜
            similarity_score = self.classifier(pooled_output)
            
            return {
                'similarity_score': similarity_score,
                'pooled_output': pooled_output
            }
            
        except Exception as e:
            print(f"âŒ Forward ì˜¤ë¥˜: {e}")
            # ë”ë¯¸ ì¶œë ¥ (300 ì°¨ì›)
            batch_size = input_ids.size(0)
            dummy_pooled = torch.zeros(batch_size, 300, device=input_ids.device)
            dummy_score = torch.zeros(batch_size, 1, device=input_ids.device)
            
            return {
                'similarity_score': dummy_score,
                'pooled_output': dummy_pooled
            }

class KcBERT300Trainer:
    """KcBERT 300 ì°¨ì› í›ˆë ¨ê¸°"""
    
    def __init__(self, model_name='beomi/kcbert-base'):
        self.model_name = model_name
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {self.device}")
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            print(f"âœ… í† í¬ë‚˜ì´ì € ë¡œë“œ ì„±ê³µ: {model_name}")
        except Exception as e:
            print(f"âŒ í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise
        
        self.model = None
    
    def load_qa_data(self, qa_dataset_path) -> List[Dict[str, Any]]:
        """QA ë°ì´í„° ë¡œë“œ"""
        try:
            with open(qa_dataset_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            print(f"âœ… QA íŒŒì¼ ë¡œë“œ ì„±ê³µ: {qa_dataset_path}")
            
            qa_data = []
            
            if isinstance(data, dict):
                if 'metadata' in data and 'qa_pairs' in data:
                    qa_data = data['qa_pairs']
                    print(f"ğŸ“ˆ ë©”íƒ€ë°ì´í„°: {data['metadata']}")
                elif 'qa_pairs' in data:
                    qa_data = data['qa_pairs']
                else:
                    for key, value in data.items():
                        if isinstance(value, list) and len(value) > 0:
                            if isinstance(value[0], dict) and 'question' in value[0]:
                                qa_data = value
                                break
            elif isinstance(data, list):
                qa_data = data
            
            # ìœ íš¨ì„± ê²€ì‚¬
            valid_qa = []
            for qa in qa_data:
                if isinstance(qa, dict) and qa.get('question') and qa.get('answer'):
                    valid_qa.append(qa)
            
            print(f"ğŸ³ ì „ì²´ ìœ íš¨í•œ QA: {len(valid_qa)}ê°œ")
            return valid_qa
            
        except Exception as e:
            print(f"âŒ QA ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return []
    
    def prepare_data(self, qa_dataset_path):
        """ë°ì´í„° ì¤€ë¹„ - 300 ê¸¸ì´ë¡œ"""
        print("ğŸš€ ì „ì²´ QA ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘ (300 ê¸¸ì´)...")
        
        qa_data = self.load_qa_data(qa_dataset_path)
        
        if not qa_data:
            raise ValueError("ìœ íš¨í•œ QA ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        print(f"ğŸ“Š ì´ {len(qa_data)}ê°œì˜ QA ìŒì„ ëª¨ë‘ ì‚¬ìš© (300 ê¸¸ì´)")
        
        # ìµœì†Œ ë°ì´í„° ìš”êµ¬ì‚¬í•­
        if len(qa_data) < 4:
            while len(qa_data) < 4:
                qa_data.extend(qa_data[:min(len(qa_data), 4-len(qa_data))])
        
        # í›ˆë ¨/ê²€ì¦ ë¶„í• 
        if len(qa_data) >= 10:
            train_data, val_data = train_test_split(qa_data, test_size=0.2, random_state=42)
        else:
            split_idx = max(1, int(len(qa_data) * 0.8))
            train_data = qa_data[:split_idx]
            val_data = qa_data[split_idx:] if split_idx < len(qa_data) else qa_data[-1:]
        
        print(f"âœ… í›ˆë ¨ ë°ì´í„°: {len(train_data)}ê°œ")
        print(f"âœ… ê²€ì¦ ë°ì´í„°: {len(val_data)}ê°œ")
        
        # ë°ì´í„°ì…‹ ìƒì„± (300 ê¸¸ì´ë¡œ)
        train_dataset = RecipeQADataset(train_data, self.tokenizer, max_length=300)
        val_dataset = RecipeQADataset(val_data, self.tokenizer, max_length=300)
        
        # ë°°ì¹˜ í¬ê¸°
        effective_batch_size = BATCH_SIZE
        print(f"ğŸ“Š ë°°ì¹˜ í¬ê¸°: {effective_batch_size}")
        
        # ë°ì´í„°ë¡œë” ìƒì„±
        train_loader = DataLoader(train_dataset, batch_size=effective_batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=effective_batch_size, shuffle=False)
        
        print(f"ğŸ¯ ì´ í›ˆë ¨ ë°°ì¹˜ ìˆ˜: {len(train_loader)}")
        print(f"ğŸ¯ ì´ ê²€ì¦ ë°°ì¹˜ ìˆ˜: {len(val_loader)}")
        
        return train_loader, val_loader
    
    def initialize_model(self):
        """ëª¨ë¸ ì´ˆê¸°í™”"""
        print("ğŸ”§ KcBERT 300 ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
        try:
            self.model = KcBERT300Model(self.model_name)
            self.model.to(self.device)
            
            total_params = sum(p.numel() for p in self.model.parameters())
            print(f"âœ… ëª¨ë¸ ì´ˆê¸°í™” ì„±ê³µ - ì´ íŒŒë¼ë¯¸í„°: {total_params:,}ê°œ")
            
            return self.model
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    def train(self, train_loader, val_loader, num_epochs=NUM_EPOCHS):
        """ëª¨ë¸ í›ˆë ¨"""
        if self.model is None:
            self.initialize_model()
        
        optimizer = AdamW(self.model.parameters(), lr=LEARNING_RATE)
        total_steps = len(train_loader) * num_epochs
        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)
        criterion = nn.MSELoss()
        
        self.model.train()
        train_losses = []
        
        print(f"\nğŸš€ KcBERT 300 í›ˆë ¨ ì‹œì‘ - {num_epochs}ê°œ ì—í¬í¬")
        print(f"ğŸ“Š ì´ í›ˆë ¨ ìŠ¤í…: {total_steps}")
        
        for epoch in range(num_epochs):
            print(f"\n{'='*50}")
            print(f"ì—í¬í¬ {epoch + 1}/{num_epochs}")
            print(f"{'='*50}")
            
            epoch_train_loss = 0
            successful_batches = 0
            failed_batches = 0
            
            train_progress = tqdm(train_loader, desc=f"ì—í¬í¬ {epoch+1} í›ˆë ¨")
            
            for batch_idx, batch in enumerate(train_progress):
                try:
                    optimizer.zero_grad()
                    
                    input_ids = batch['input_ids'].to(self.device)
                    attention_mask = batch['attention_mask'].to(self.device)
                    
                    # ì…ë ¥ í¬ê¸° í™•ì¸
                    print(f"ğŸ” ì…ë ¥ í¬ê¸°: {input_ids.shape}")
                    
                    current_batch_size = input_ids.shape[0]
                    
                    # ìˆœì „íŒŒ
                    outputs = self.model(input_ids, attention_mask)
                    similarity_score = outputs['similarity_score']
                    
                    # íƒ€ê²Ÿ ìƒì„±
                    target = torch.ones(current_batch_size, 1).to(self.device)
                    
                    # í¬ê¸° ê²€ì¦
                    print(f"ğŸ” ì¶œë ¥ í¬ê¸°: {similarity_score.shape}, íƒ€ê²Ÿ í¬ê¸°: {target.shape}")
                    
                    if similarity_score.shape != target.shape:
                        if similarity_score.dim() == 1:
                            similarity_score = similarity_score.unsqueeze(1)
                    
                    # ì†ì‹¤ ê³„ì‚°
                    loss = criterion(similarity_score, target)
                    
                    # ì—­ì „íŒŒ
                    loss.backward()
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                    optimizer.step()
                    scheduler.step()
                    
                    epoch_train_loss += loss.item()
                    successful_batches += 1
                    
                    train_progress.set_postfix({
                        'loss': f"{loss.item():.4f}",
                        'success': f"{successful_batches}/{batch_idx+1}"
                    })
                    
                except Exception as e:
                    failed_batches += 1
                    print(f"\nâŒ ë°°ì¹˜ {batch_idx} ì˜¤ë¥˜: {e}")
                    if failed_batches > 10:  # 10ê°œ ì´ìƒ ì‹¤íŒ¨í•˜ë©´ ì¤‘ë‹¨
                        print("ë„ˆë¬´ ë§ì€ ë°°ì¹˜ ì‹¤íŒ¨, í›ˆë ¨ ì¤‘ë‹¨")
                        break
                    continue
            
            avg_train_loss = epoch_train_loss / successful_batches if successful_batches > 0 else 0
            train_losses.append(avg_train_loss)
            
            print(f"\nğŸ“Š ì—í¬í¬ {epoch+1} ê²°ê³¼:")
            print(f"   ì„±ê³µí•œ ë°°ì¹˜: {successful_batches}/{len(train_loader)}")
            print(f"   ì‹¤íŒ¨í•œ ë°°ì¹˜: {failed_batches}/{len(train_loader)}")
            print(f"   í›ˆë ¨ ì†ì‹¤: {avg_train_loss:.4f}")
            
            if successful_batches == 0:
                print("âŒ ì„±ê³µí•œ ë°°ì¹˜ê°€ ì—†ìŠµë‹ˆë‹¤. í›ˆë ¨ ì¤‘ë‹¨.")
                break
        
        return train_losses, []
    
    def save_model(self, save_path):
        """ëª¨ë¸ ì €ì¥"""
        print(f"ğŸ“ ëª¨ë¸ ì €ì¥ ì¤‘: {save_path}")
        
        try:
            save_path.mkdir(parents=True, exist_ok=True)
            
            torch.save(self.model.state_dict(), save_path / "pytorch_model.bin")
            
            config = {
                'model_name': self.model_name,
                'max_length': 300,
                'hidden_size': 300,
                'model_type': 'KcBERT300Model'
            }
            with open(save_path / "config.json", 'w', encoding='utf-8') as f:
                json.dump(config, f, ensure_ascii=False, indent=2)
            
            tokenizer_path = save_path / "tokenizer"
            tokenizer_path.mkdir(exist_ok=True)
            self.tokenizer.save_pretrained(tokenizer_path)
            
            print("âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ!")
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}")

def main():
    """ë©”ì¸ í›ˆë ¨ í•¨ìˆ˜"""
    print("ğŸš€ KcBERT 300 ì°¨ì› ëª¨ë¸ í›ˆë ¨ ì‹œì‘!")
    print("ğŸ“ ëª¨ë“  ì„¤ì •ì„ 300ìœ¼ë¡œ í†µì¼")
    
    if not QA_DATASET_PATH.exists():
        print(f"âŒ QA ë°ì´í„°ì…‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {QA_DATASET_PATH}")
        return
    
    try:
        trainer = KcBERT300Trainer(MODEL_NAME)
        train_loader, val_loader = trainer.prepare_data(QA_DATASET_PATH)
        
        print("\nğŸ¯ KcBERT 300 ì°¨ì›ìœ¼ë¡œ í›ˆë ¨ ì‹œì‘!")
        train_losses, _ = trainer.train(train_loader, val_loader, NUM_EPOCHS)
        
        trainer.save_model(TRAINED_MODEL_DIR)
        
        print(f"\nğŸ‰ KcBERT 300 í›ˆë ¨ ì™„ë£Œ!")
        if train_losses:
            print(f"âœ… ìµœì¢… í›ˆë ¨ ì†ì‹¤: {train_losses[-1]:.4f}")
        
    except Exception as e:
        print(f"âŒ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()