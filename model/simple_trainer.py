"""
ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ìš© ëª¨ë¸ í›ˆë ¨ê¸°
- ë©”ëª¨ë¦¬ ìµœì í™”
- ë””ë²„ê¹… ì¶œë ¥ ê°•í™”
- ë‹¨ê³„ë³„ ì§„í–‰ í™•ì¸
"""
import json
import torch
import torch.nn as nn
from torch.optim import AdamW
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, AutoModel
import numpy as np
from tqdm import tqdm
import sys
import os
import time
from typing import List, Dict, Any
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from config import *

class SimpleQADataset(Dataset):
    """ê°„ë‹¨í•œ QA ë°ì´í„°ì…‹"""
    
    def __init__(self, qa_data: List[Dict[str, Any]], tokenizer, max_length: int = 128):
        print(f"ğŸ“Š ë°ì´í„°ì…‹ ì´ˆê¸°í™” ì¤‘... (ìµœëŒ€ ê¸¸ì´: {max_length})")
        
        self.qa_data = qa_data[:100]  # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ 100ê°œë§Œ ì‚¬ìš©
        self.tokenizer = tokenizer
        self.max_length = max_length
        
        print(f"âœ… ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ: {len(self.qa_data)}ê°œ QA ìŒ")
    
    def __len__(self):
        return len(self.qa_data)
    
    def __getitem__(self, idx):
        qa = self.qa_data[idx]
        question = str(qa.get('question', '')).strip()
        answer = str(qa.get('answer', '')).strip()
        
        # ê°„ë‹¨í•œ ì¸ì½”ë”©
        encoding = self.tokenizer(
            question,
            add_special_tokens=True,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'question': question,
            'answer': answer
        }

class SimpleRecipeModel(nn.Module):
    """ê°„ë‹¨í•œ ë ˆì‹œí”¼ ëª¨ë¸"""
    
    def __init__(self, model_name: str = "beomi/kcbert-base"):
        super(SimpleRecipeModel, self).__init__()
        
        print(f"ğŸ”§ ëª¨ë¸ ì´ˆê¸°í™”: {model_name}")
        
        try:
            # ëª¨ë¸ í¬ê¸° ì¤„ì´ê¸°
            self.bert = AutoModel.from_pretrained(
                model_name,
                output_hidden_states=False,
                output_attentions=False
            )
            print("âœ… BERT ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ BERT ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            print("ğŸ”„ ë” ì‘ì€ ëª¨ë¸ë¡œ ëŒ€ì²´...")
            self.bert = AutoModel.from_pretrained("klue/bert-base")
        
        # ê°„ë‹¨í•œ ë¶„ë¥˜ í—¤ë“œ
        self.classifier = nn.Sequential(
            nn.Dropout(0.1),
            nn.Linear(self.bert.config.hidden_size, 256),
            nn.ReLU(),
            nn.Linear(256, 1)
        )
        
        print("âœ… ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def forward(self, input_ids, attention_mask):
        outputs = self.bert(
            input_ids=input_ids,
            attention_mask=attention_mask,
            return_dict=True
        )
        
        pooled_output = outputs.pooler_output
        logits = self.classifier(pooled_output)
        
        return logits

class SimpleTrainer:
    """ê°„ë‹¨í•œ í›ˆë ¨ê¸°"""
    
    def __init__(self, model_name: str = "beomi/kcbert-base"):
        print("ğŸš€ ê°„ë‹¨í•œ í›ˆë ¨ê¸° ì´ˆê¸°í™” ì¤‘...")
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ğŸ”§ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {self.device}")
        
        if self.device.type == 'cuda':
            print(f"   GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB")
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        try:
            print(f"ğŸ“¥ í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘: {model_name}")
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.unk_token
            print("âœ… í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹¤íŒ¨: {e}")
            print("ğŸ”„ ê¸°ë³¸ í† í¬ë‚˜ì´ì € ì‚¬ìš©")
            self.tokenizer = AutoTokenizer.from_pretrained("klue/bert-base")
        
        self.model_name = model_name
        self.model = None
        self.train_dataset = None
        
    def load_qa_data(self, qa_dataset_path: str) -> List[Dict[str, Any]]:
        """QA ë°ì´í„° ë¡œë“œ"""
        print(f"ğŸ“‚ QA ë°ì´í„° ë¡œë“œ ì¤‘: {qa_dataset_path}")
        
        try:
            with open(qa_dataset_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            qa_data = []
            if isinstance(data, dict) and 'qa_pairs' in data:
                qa_data = data['qa_pairs']
                print(f"ğŸ“ˆ ë©”íƒ€ë°ì´í„° í™•ì¸ë¨")
            elif isinstance(data, list):
                qa_data = data
            
            # ìœ íš¨ì„± ê²€ì‚¬ ë° í•„í„°ë§
            valid_qa = []
            for qa in qa_data:
                if (isinstance(qa, dict) and 
                    qa.get('question') and 
                    qa.get('answer') and
                    len(str(qa['question']).strip()) >= 3 and
                    len(str(qa['answer']).strip()) >= 5):
                    valid_qa.append(qa)
            
            print(f"âœ… ìœ íš¨í•œ QA: {len(valid_qa)}ê°œ")
            
            # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ì²˜ìŒ 200ê°œë§Œ ì‚¬ìš©
            test_qa = valid_qa[:200]
            print(f"ğŸ§ª í…ŒìŠ¤íŠ¸ìš© QA: {len(test_qa)}ê°œ ì‚¬ìš©")
            
            return test_qa
            
        except Exception as e:
            print(f"âŒ QA ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return []
    
    def prepare_data(self, qa_dataset_path: str):
        """ë°ì´í„° ì¤€ë¹„"""
        print("ğŸš€ ë°ì´í„° ì¤€ë¹„ ì‹œì‘...")
        
        qa_data = self.load_qa_data(qa_dataset_path)
        
        if not qa_data:
            raise ValueError("ìœ íš¨í•œ QA ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # ê°„ë‹¨í•œ ë¶„í•  (80:20)
        split_idx = int(len(qa_data) * 0.8)
        train_data = qa_data[:split_idx]
        val_data = qa_data[split_idx:]
        
        print(f"âœ… í›ˆë ¨ ë°ì´í„°: {len(train_data)}ê°œ")
        print(f"âœ… ê²€ì¦ ë°ì´í„°: {len(val_data)}ê°œ")
        
        # ë°ì´í„°ì…‹ ìƒì„± (ë” ì‘ì€ max_length ì‚¬ìš©)
        self.train_dataset = SimpleQADataset(train_data, self.tokenizer, max_length=128)
        self.val_dataset = SimpleQADataset(val_data, self.tokenizer, max_length=128)
        
        return self.train_dataset, self.val_dataset
    
    def initialize_model(self):
        """ëª¨ë¸ ì´ˆê¸°í™”"""
        print("ğŸ”§ ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
        
        try:
            self.model = SimpleRecipeModel(self.model_name)
            self.model.to(self.device)
            
            # ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°
            total_params = sum(p.numel() for p in self.model.parameters())
            trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
            
            print(f"âœ… ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
            print(f"   ì´ íŒŒë¼ë¯¸í„°: {total_params:,}")
            print(f"   í›ˆë ¨ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°: {trainable_params:,}")
            
            return self.model
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    def train(self, num_epochs: int = 1, batch_size: int = 4, learning_rate: float = 5e-5):
        """ê°„ë‹¨í•œ í›ˆë ¨"""
        print("ğŸš€ ê°„ë‹¨í•œ í›ˆë ¨ ì‹œì‘...")
        
        if self.model is None:
            self.initialize_model()
        
        if self.train_dataset is None:
            raise ValueError("í›ˆë ¨ ë°ì´í„°ê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # ì‘ì€ ë°°ì¹˜ í¬ê¸° ì‚¬ìš©
        train_loader = DataLoader(
            self.train_dataset, 
            batch_size=batch_size, 
            shuffle=True,
            num_workers=0  # ìœˆë„ìš° í˜¸í™˜ì„±
        )
        
        val_loader = DataLoader(
            self.val_dataset, 
            batch_size=batch_size, 
            shuffle=False,
            num_workers=0
        )
        
        # ì˜µí‹°ë§ˆì´ì €
        optimizer = AdamW(
            self.model.parameters(), 
            lr=learning_rate,
            weight_decay=0.01
        )
        
        # ì†ì‹¤ í•¨ìˆ˜
        criterion = nn.BCEWithLogitsLoss()
        
        print(f"ğŸ“Š í›ˆë ¨ ì„¤ì •:")
        print(f"   ì—í¬í¬: {num_epochs}")
        print(f"   ë°°ì¹˜ í¬ê¸°: {batch_size}")
        print(f"   í•™ìŠµë¥ : {learning_rate}")
        print(f"   í›ˆë ¨ ë°°ì¹˜ ìˆ˜: {len(train_loader)}")
        print(f"   ê²€ì¦ ë°°ì¹˜ ìˆ˜: {len(val_loader)}")
        
        for epoch in range(num_epochs):
            print(f"\n{'='*50}")
            print(f"ì—í¬í¬ {epoch + 1}/{num_epochs}")
            print(f"{'='*50}")
            
            # í›ˆë ¨
            self.model.train()
            total_loss = 0
            
            print("ğŸƒâ€â™‚ï¸ í›ˆë ¨ ì‹œì‘...")
            for batch_idx, batch in enumerate(tqdm(train_loader, desc="í›ˆë ¨")):
                try:
                    optimizer.zero_grad()
                    
                    input_ids = batch['input_ids'].to(self.device)
                    attention_mask = batch['attention_mask'].to(self.device)
                    
                    # ìˆœì „íŒŒ
                    logits = self.model(input_ids, attention_mask)
                    
                    # íƒ€ê²Ÿ (ëª¨ë“  QAë¥¼ positiveë¡œ)
                    targets = torch.ones(input_ids.size(0), 1).to(self.device)
                    
                    # ì†ì‹¤ ê³„ì‚°
                    loss = criterion(logits, targets)
                    
                    # ì—­ì „íŒŒ
                    loss.backward()
                    
                    # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                    
                    optimizer.step()
                    
                    total_loss += loss.item()
                    
                    # ë©”ëª¨ë¦¬ ì •ë¦¬
                    if self.device.type == 'cuda':
                        torch.cuda.empty_cache()
                    
                except Exception as e:
                    print(f"\nâŒ ë°°ì¹˜ {batch_idx} í›ˆë ¨ ì˜¤ë¥˜: {e}")
                    continue
            
            avg_loss = total_loss / len(train_loader)
            print(f"ğŸ¯ í‰ê·  í›ˆë ¨ ì†ì‹¤: {avg_loss:.4f}")
            
            # ê²€ì¦ (ê°„ë‹¨íˆ)
            self.model.eval()
            val_loss = 0
            
            print("ğŸ“Š ê²€ì¦ ì‹œì‘...")
            with torch.no_grad():
                for batch in tqdm(val_loader, desc="ê²€ì¦"):
                    try:
                        input_ids = batch['input_ids'].to(self.device)
                        attention_mask = batch['attention_mask'].to(self.device)
                        
                        logits = self.model(input_ids, attention_mask)
                        targets = torch.ones(input_ids.size(0), 1).to(self.device)
                        
                        loss = criterion(logits, targets)
                        val_loss += loss.item()
                        
                    except Exception as e:
                        print(f"âŒ ê²€ì¦ ì˜¤ë¥˜: {e}")
                        continue
            
            avg_val_loss = val_loss / len(val_loader) if len(val_loader) > 0 else 0
            print(f"ğŸ“Š í‰ê·  ê²€ì¦ ì†ì‹¤: {avg_val_loss:.4f}")
        
        print(f"\nğŸ‰ í›ˆë ¨ ì™„ë£Œ!")
        return True
    
    def save_model(self, save_path):
        """ëª¨ë¸ ì €ì¥ (ê°„ë‹¨ ë²„ì „)"""
        print(f"ğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘: {save_path}")
        
        try:
            save_path.mkdir(parents=True, exist_ok=True)
            
            # ëª¨ë¸ ê°€ì¤‘ì¹˜ë§Œ ì €ì¥
            torch.save(self.model.state_dict(), save_path / "pytorch_model.bin")
            
            # ê°„ë‹¨í•œ ì„¤ì • ì €ì¥
            config = {
                'model_name': self.model_name,
                'model_type': 'SimpleRecipeModel',
                'training_completed': True,
                'note': 'simple_test_model'
            }
            
            with open(save_path / "config.json", 'w', encoding='utf-8') as f:
                json.dump(config, f, ensure_ascii=False, indent=2)
            
            print("âœ… ê°„ë‹¨í•œ ëª¨ë¸ ì €ì¥ ì™„ë£Œ!")
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}")

def main():
    """ë©”ì¸ í›ˆë ¨ í•¨ìˆ˜"""
    print("ğŸš€ ê°„ë‹¨í•œ ë ˆì‹œí”¼ ì±—ë´‡ ëª¨ë¸ í›ˆë ¨ ì‹œì‘!")
    print("âš¡ ì´ ë²„ì „ì€ í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ë¹ ë¥´ê²Œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
    
    # ì§„í–‰ ìƒí™© ì¶œë ¥
    def print_progress(message):
        print(f"\n{'='*60}")
        print(f"ğŸ“ {message}")
        print(f"{'='*60}")
        time.sleep(1)  # í™•ì¸ìš© ëŒ€ê¸°
    
    print_progress("1ë‹¨ê³„: QA ë°ì´í„° í™•ì¸")
    
    if not QA_DATASET_PATH.exists():
        print(f"âŒ QA ë°ì´í„°ì…‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {QA_DATASET_PATH}")
        print("ë¨¼ì € enhanced_qa_generator.pyë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        return
    
    try:
        print_progress("2ë‹¨ê³„: í›ˆë ¨ê¸° ì´ˆê¸°í™”")
        trainer = SimpleTrainer(MODEL_NAME)
        
        print_progress("3ë‹¨ê³„: ë°ì´í„° ì¤€ë¹„")
        trainer.prepare_data(QA_DATASET_PATH)
        
        print_progress("4ë‹¨ê³„: ëª¨ë¸ í›ˆë ¨")
        success = trainer.train(
            num_epochs=1,     # 1 ì—í¬í¬ë§Œ
            batch_size=4,     # ì‘ì€ ë°°ì¹˜
            learning_rate=5e-5
        )
        
        if success:
            print_progress("5ë‹¨ê³„: ëª¨ë¸ ì €ì¥")
            trainer.save_model(TRAINED_MODEL_DIR)
            print(f"\nğŸ‰ ê°„ë‹¨í•œ í›ˆë ¨ ì™„ë£Œ!")
        
    except Exception as e:
        print(f"âŒ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()