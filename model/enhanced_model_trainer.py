"""
ë””ë²„ê¹… ê°•í™”ëœ ë ˆì‹œí”¼ ì±—ë´‡ ëª¨ë¸ í›ˆë ¨ê¸°
- ìƒì„¸í•œ ì§„í–‰ ìƒí™© ì¶œë ¥
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
- ì—ëŸ¬ ìƒì„¸ ë¶„ì„
"""
import json
import torch
import torch.nn as nn
from torch.optim import AdamW
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from transformers import (
    AutoTokenizer, AutoModel, 
    get_linear_schedule_with_warmup,
    AutoConfig
)
from sklearn.model_selection import train_test_split
import numpy as np
from tqdm import tqdm
import sys
import os
from typing import List, Dict, Any, Tuple
import random
import time
import psutil
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from config import *

def print_memory_usage():
    """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶œë ¥"""
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1e9
        cached = torch.cuda.memory_reserved() / 1e9
        print(f"ğŸ”§ GPU ë©”ëª¨ë¦¬: {allocated:.1f}GB allocated, {cached:.1f}GB cached")
    
    ram = psutil.virtual_memory()
    print(f"ğŸ”§ RAM ì‚¬ìš©: {ram.percent:.1f}% ({ram.used / 1e9:.1f}GB / {ram.total / 1e9:.1f}GB)")

def print_step(step_num, message, detail=""):
    """ë‹¨ê³„ë³„ ì§„í–‰ ìƒí™© ì¶œë ¥"""
    print(f"\n{'='*70}")
    print(f"ğŸ“ {step_num}: {message}")
    if detail:
        print(f"   {detail}")
    print(f"{'='*70}")
    print_memory_usage()
    time.sleep(1)

class DebugQADataset(Dataset):
    """ë””ë²„ê¹…ì´ ê°•í™”ëœ QA ë°ì´í„°ì…‹"""
    
    def __init__(self, qa_data: List[Dict[str, Any]], tokenizer, max_length: int = 200):
        print_step("ë°ì´í„°ì…‹ ì´ˆê¸°í™”", f"ì´ {len(qa_data)}ê°œ QA ì²˜ë¦¬ ì¤‘...")
        
        self.tokenizer = tokenizer
        self.max_length = max_length
        
        print("ğŸ” ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬ ì¤‘...")
        valid_qa = []
        for i, qa in enumerate(qa_data):
            if isinstance(qa, dict) and qa.get('question') and qa.get('answer'):
                question = str(qa['question']).strip()
                answer = str(qa['answer']).strip()
                
                if len(question) >= 3 and len(answer) >= 5 and len(answer) <= 500:
                    valid_qa.append(qa)
            
            if i % 1000 == 0:
                print(f"   ì²˜ë¦¬ ì¤‘: {i}/{len(qa_data)} ({i/len(qa_data)*100:.1f}%)")
        
        self.qa_data = valid_qa
        print(f"âœ… ìœ íš¨í•œ QA: {len(self.qa_data)}ê°œ (ì›ë³¸ì˜ {len(self.qa_data)/len(qa_data)*100:.1f}%)")
        
        # ë°ì´í„° í†µê³„
        question_lengths = [len(qa['question']) for qa in self.qa_data[:100]]
        answer_lengths = [len(qa['answer']) for qa in self.qa_data[:100]]
        
        print(f"ğŸ“Š ë°ì´í„° í†µê³„ (ìƒ˜í”Œ 100ê°œ):")
        print(f"   í‰ê·  ì§ˆë¬¸ ê¸¸ì´: {np.mean(question_lengths):.1f}ì")
        print(f"   í‰ê·  ë‹µë³€ ê¸¸ì´: {np.mean(answer_lengths):.1f}ì")
    
    def __len__(self):
        return len(self.qa_data)
    
    def __getitem__(self, idx):
        qa = self.qa_data[idx]
        question = str(qa.get('question', '')).strip()
        answer = str(qa.get('answer', '')).strip()
        
        # ì§ˆë¬¸ê³¼ ë‹µë³€ì„ í•¨ê»˜ ì¸ì½”ë”©
        try:
            encoding = self.tokenizer(
                question,
                answer,
                add_special_tokens=True,
                max_length=self.max_length,
                padding='max_length',
                truncation=True,
                return_tensors='pt'
            )
            
            return {
                'input_ids': encoding['input_ids'].flatten(),
                'attention_mask': encoding['attention_mask'].flatten(),
                'token_type_ids': encoding.get('token_type_ids', torch.zeros_like(encoding['input_ids'])).flatten(),
                'question': question,
                'answer': answer
            }
        except Exception as e:
            print(f"âš ï¸ í† í¬ë‚˜ì´ì§• ì˜¤ë¥˜ (idx {idx}): {e}")
            # ê¸°ë³¸ê°’ ë°˜í™˜
            return {
                'input_ids': torch.zeros(self.max_length, dtype=torch.long),
                'attention_mask': torch.zeros(self.max_length, dtype=torch.long),
                'token_type_ids': torch.zeros(self.max_length, dtype=torch.long),
                'question': question,
                'answer': answer
            }

class DebugRecipeChatbotModel(nn.Module):
    """ë””ë²„ê¹…ì´ ê°•í™”ëœ ë ˆì‹œí”¼ ì±—ë´‡ ëª¨ë¸"""
    
    def __init__(self, model_name: str = "beomi/kcbert-base", hidden_dropout_prob: float = 0.1):
        super(DebugRecipeChatbotModel, self).__init__()
        
        print_step("ëª¨ë¸ ì´ˆê¸°í™”", f"ëª¨ë¸: {model_name}")
        
        try:
            print("ğŸ“¥ ì„¤ì • ë¡œë“œ ì¤‘...")
            self.config = AutoConfig.from_pretrained(model_name)
            self.config.hidden_dropout_prob = hidden_dropout_prob
            print(f"âœ… ì„¤ì • ë¡œë“œ ì™„ë£Œ - Hidden size: {self.config.hidden_size}")
            
            print("ğŸ“¥ BERT ëª¨ë¸ ë¡œë“œ ì¤‘...")
            self.bert = AutoModel.from_pretrained(model_name, config=self.config, ignore_mismatched_sizes=True)
            print("âœ… BERT ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            print("ğŸ”„ ë” ì‘ì€ ëª¨ë¸ë¡œ í´ë°±...")
            try:
                self.config = AutoConfig.from_pretrained("klue/bert-base")
                self.bert = AutoModel.from_pretrained("klue/bert-base")
                print("âœ… í´ë°± ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            except Exception as e2:
                print(f"âŒ í´ë°± ëª¨ë¸ë„ ì‹¤íŒ¨: {e2}")
                raise
        
        # ëª¨ë¸ ì°¨ì›
        self.hidden_size = self.bert.config.hidden_size
        print(f"ğŸ“ Hidden size: {self.hidden_size}")
        
        # QA ë§¤ì¹­ì„ ìœ„í•œ í—¤ë“œ
        print("ğŸ”§ ë¶„ë¥˜ í—¤ë“œ ì´ˆê¸°í™”...")
        self.qa_classifier = nn.Sequential(
            nn.Dropout(hidden_dropout_prob),
            nn.Linear(self.hidden_size, self.hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(hidden_dropout_prob),
            nn.Linear(self.hidden_size // 2, 1)
        )
        
        # ì„ë² ë”© ìƒì„±ì„ ìœ„í•œ í”„ë¡œì ì…˜ í—¤ë“œ
        self.embedding_projection = nn.Sequential(
            nn.Dropout(hidden_dropout_prob),
            nn.Linear(self.hidden_size, self.hidden_size),
            nn.Tanh()
        )
        
        print("âœ… ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def forward(self, input_ids, attention_mask, token_type_ids=None):
        """ìˆœì „íŒŒ"""
        try:
            outputs = self.bert(
                input_ids=input_ids,
                attention_mask=attention_mask,
                token_type_ids=token_type_ids,
                return_dict=True
            )
            
            pooled_output = outputs.pooler_output
            qa_logits = self.qa_classifier(pooled_output)
            embeddings = self.embedding_projection(pooled_output)
            
            return {
                'qa_logits': qa_logits,
                'embeddings': embeddings,
                'pooled_output': pooled_output
            }
        except Exception as e:
            print(f"âŒ ìˆœì „íŒŒ ì˜¤ë¥˜: {e}")
            raise

class DebugModelTrainer:
    """ë””ë²„ê¹…ì´ ê°•í™”ëœ ëª¨ë¸ í›ˆë ¨ê¸°"""
    
    def __init__(self, model_name: str = "beomi/kcbert-base"):
        print_step("í›ˆë ¨ê¸° ì´ˆê¸°í™”", f"ëª¨ë¸: {model_name}")
        
        self.model_name = model_name
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        print(f"ğŸ”§ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {self.device}")
        if self.device.type == 'cuda':
            print(f"   GPU: {torch.cuda.get_device_name(0)}")
            print(f"   ì´ GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB")
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        try:
            print("ğŸ“¥ í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘...")
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.unk_token
            print(f"âœ… í† í¬ë‚˜ì´ì € ë¡œë“œ ì„±ê³µ - Vocab size: {len(self.tokenizer)}")
        except Exception as e:
            print(f"âŒ í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹¤íŒ¨: {e}")
            print("ğŸ”„ ê¸°ë³¸ í† í¬ë‚˜ì´ì € ì‚¬ìš©...")
            self.tokenizer = AutoTokenizer.from_pretrained("klue/bert-base")
        
        self.model = None
        self.train_dataset = None
        self.val_dataset = None
    
    def load_qa_data(self, qa_dataset_path: str) -> List[Dict[str, Any]]:
        """QA ë°ì´í„° ë¡œë“œ"""
        print_step("QA ë°ì´í„° ë¡œë“œ", f"íŒŒì¼: {qa_dataset_path}")
        
        try:
            # íŒŒì¼ í¬ê¸° í™•ì¸
            file_size = os.path.getsize(qa_dataset_path) / (1024 * 1024)
            print(f"ğŸ“Š íŒŒì¼ í¬ê¸°: {file_size:.1f}MB")
            
            print("ğŸ“‚ JSON íŒŒì¼ ë¡œë“œ ì¤‘...")
            with open(qa_dataset_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            print("âœ… JSON íŒŒì¼ ë¡œë“œ ì™„ë£Œ")
            
            qa_data = []
            if isinstance(data, dict):
                if 'qa_pairs' in data:
                    qa_data = data['qa_pairs']
                    if 'metadata' in data:
                        metadata = data['metadata']
                        print(f"ğŸ“ˆ ë©”íƒ€ë°ì´í„°:")
                        for key, value in metadata.items():
                            print(f"   {key}: {value}")
                    if 'statistics' in data:
                        print(f"ğŸ“Š í†µê³„ ì •ë³´ í¬í•¨")
                else:
                    for key, value in data.items():
                        if isinstance(value, list) and len(value) > 0:
                            if isinstance(value[0], dict) and 'question' in value[0]:
                                qa_data = value
                                break
            elif isinstance(data, list):
                qa_data = data
            
            print(f"ğŸ“Š ë¡œë“œëœ QA ìˆ˜: {len(qa_data)}ê°œ")
            
            # ë°ì´í„° ìƒ˜í”Œ í™•ì¸
            if qa_data:
                sample = qa_data[0]
                print(f"ğŸ“‹ ìƒ˜í”Œ QA:")
                print(f"   ì§ˆë¬¸: {sample.get('question', '')[:50]}...")
                print(f"   ë‹µë³€: {sample.get('answer', '')[:50]}...")
                print(f"   íƒ€ì…: {sample.get('type', 'N/A')}")
            
            return qa_data
            
        except Exception as e:
            print(f"âŒ QA ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    def prepare_data(self, qa_dataset_path: str, test_size: float = 0.2):
        """ë°ì´í„° ì¤€ë¹„"""
        print_step("ë°ì´í„° ì¤€ë¹„", "í›ˆë ¨/ê²€ì¦ ë°ì´í„° ë¶„í• ")
        
        qa_data = self.load_qa_data(qa_dataset_path)
        
        if not qa_data:
            raise ValueError("ìœ íš¨í•œ QA ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        print(f"ğŸ”€ ë°ì´í„° ì…”í”Œ ì¤‘...")
        random.shuffle(qa_data)
        
        # í›ˆë ¨/ê²€ì¦ ë¶„í• 
        if len(qa_data) >= 10:
            train_data, val_data = train_test_split(
                qa_data, 
                test_size=test_size, 
                random_state=42
            )
        else:
            split_idx = max(1, int(len(qa_data) * 0.8))
            train_data = qa_data[:split_idx]
            val_data = qa_data[split_idx:] if split_idx < len(qa_data) else qa_data[-1:]
        
        print(f"âœ… ë°ì´í„° ë¶„í•  ì™„ë£Œ:")
        print(f"   í›ˆë ¨ ë°ì´í„°: {len(train_data)}ê°œ")
        print(f"   ê²€ì¦ ë°ì´í„°: {len(val_data)}ê°œ")
        
        # ë°ì´í„°ì…‹ ìƒì„±
        print("ğŸ”§ PyTorch ë°ì´í„°ì…‹ ìƒì„± ì¤‘...")
        self.train_dataset = DebugQADataset(train_data, self.tokenizer, max_length=200)
        self.val_dataset = DebugQADataset(val_data, self.tokenizer, max_length=200)
        
        return self.train_dataset, self.val_dataset
    
    def initialize_model(self):
        """ëª¨ë¸ ì´ˆê¸°í™”"""
        print_step("ëª¨ë¸ ì´ˆê¸°í™”", "GPU ë©”ëª¨ë¦¬ë¡œ ì´ë™")
        
        try:
            self.model = DebugRecipeChatbotModel(self.model_name)
            
            # ëª¨ë¸ì„ GPUë¡œ ì´ë™
            print("ğŸš€ ëª¨ë¸ì„ GPUë¡œ ì´ë™ ì¤‘...")
            self.model.to(self.device)
            
            # ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°
            total_params = sum(p.numel() for p in self.model.parameters())
            trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
            
            print(f"âœ… ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
            print(f"   ì´ íŒŒë¼ë¯¸í„°: {total_params:,}")
            print(f"   í›ˆë ¨ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°: {trainable_params:,}")
            print(f"   ëª¨ë¸ í¬ê¸°: {total_params * 4 / 1e9:.2f}GB")
            
            print_memory_usage()
            
            return self.model
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            raise
    
    def train(self, num_epochs: int = 10, batch_size: int = 8, learning_rate: float = 5e-5):
        """ëª¨ë¸ í›ˆë ¨"""
        print_step("í›ˆë ¨ ì‹œì‘", f"ì—í¬í¬: {num_epochs}, ë°°ì¹˜: {batch_size}, í•™ìŠµë¥ : {learning_rate}")
        
        if self.model is None:
            self.initialize_model()
        
        if self.train_dataset is None:
            raise ValueError("í›ˆë ¨ ë°ì´í„°ê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # ë°ì´í„°ë¡œë” ìƒì„±
        print("ğŸ”§ ë°ì´í„°ë¡œë” ìƒì„± ì¤‘...")
        train_loader = DataLoader(
            self.train_dataset, 
            batch_size=batch_size, 
            shuffle=True,
            num_workers=0,
            pin_memory=True if self.device.type == 'cuda' else False
        )
        
        val_loader = DataLoader(
            self.val_dataset, 
            batch_size=batch_size, 
            shuffle=False,
            num_workers=0,
            pin_memory=True if self.device.type == 'cuda' else False
        )
        
        print(f"âœ… ë°ì´í„°ë¡œë” ìƒì„± ì™„ë£Œ:")
        print(f"   í›ˆë ¨ ë°°ì¹˜ ìˆ˜: {len(train_loader)}")
        print(f"   ê²€ì¦ ë°°ì¹˜ ìˆ˜: {len(val_loader)}")
        
        # ì˜µí‹°ë§ˆì´ì €ì™€ ìŠ¤ì¼€ì¤„ëŸ¬
        print("ğŸ”§ ì˜µí‹°ë§ˆì´ì € ì„¤ì • ì¤‘...")
        optimizer = AdamW(
            self.model.parameters(), 
            lr=learning_rate,
            weight_decay=0.01,
            eps=1e-8
        )
        
        total_steps = len(train_loader) * num_epochs
        scheduler = get_linear_schedule_with_warmup(
            optimizer,
            num_warmup_steps=int(0.1 * total_steps),
            num_training_steps=total_steps
        )
        
        criterion = nn.BCEWithLogitsLoss()
        
        print(f"âœ… í›ˆë ¨ ì„¤ì • ì™„ë£Œ:")
        print(f"   ì´ ìŠ¤í…: {total_steps}")
        print(f"   ì›œì—… ìŠ¤í…: {int(0.1 * total_steps)}")
        
        best_val_loss = float('inf')
        train_losses = []
        val_losses = []
        
        for epoch in range(num_epochs):
            print_step(f"ì—í¬í¬ {epoch + 1}/{num_epochs}", "í›ˆë ¨ ë° ê²€ì¦")
            
            # í›ˆë ¨ ë‹¨ê³„
            self.model.train()
            total_train_loss = 0
            successful_batches = 0
            
            print("ğŸƒâ€â™‚ï¸ í›ˆë ¨ ì¤‘...")
            train_progress = tqdm(train_loader, desc=f"í›ˆë ¨ ì—í¬í¬ {epoch+1}")
            
            for batch_idx, batch in enumerate(train_progress):
                try:
                    optimizer.zero_grad()
                    
                    # ë°ì´í„° GPUë¡œ ì´ë™
                    input_ids = batch['input_ids'].to(self.device)
                    attention_mask = batch['attention_mask'].to(self.device)
                    token_type_ids = batch['token_type_ids'].to(self.device)
                    
                    # ìˆœì „íŒŒ
                    outputs = self.model(
                        input_ids=input_ids,
                        attention_mask=attention_mask,
                        token_type_ids=token_type_ids
                    )
                    
                    # íƒ€ê²Ÿ ìƒì„±
                    batch_size_current = input_ids.size(0)
                    targets = torch.ones(batch_size_current, 1).to(self.device)
                    
                    # ì†ì‹¤ ê³„ì‚°
                    qa_logits = outputs['qa_logits']
                    loss = criterion(qa_logits, targets)
                    
                    # ì—­ì „íŒŒ
                    loss.backward()
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                    optimizer.step()
                    scheduler.step()
                    
                    total_train_loss += loss.item()
                    successful_batches += 1
                    
                    train_progress.set_postfix({
                        'loss': f"{loss.item():.4f}",
                        'lr': f"{scheduler.get_last_lr()[0]:.2e}",
                        'mem': f"{torch.cuda.memory_allocated() / 1e9:.1f}GB" if torch.cuda.is_available() else "N/A"
                    })
                    
                    # ë©”ëª¨ë¦¬ ì •ë¦¬
                    if self.device.type == 'cuda' and batch_idx % 10 == 0:
                        torch.cuda.empty_cache()
                    
                except Exception as e:
                    print(f"\nâŒ ë°°ì¹˜ {batch_idx} í›ˆë ¨ ì˜¤ë¥˜: {e}")
                    continue
            
            avg_train_loss = total_train_loss / successful_batches if successful_batches > 0 else float('inf')
            train_losses.append(avg_train_loss)
            
            print(f"âœ… í›ˆë ¨ ì™„ë£Œ - í‰ê·  ì†ì‹¤: {avg_train_loss:.4f} (ì„±ê³µí•œ ë°°ì¹˜: {successful_batches}/{len(train_loader)})")
            
            # ê²€ì¦ ë‹¨ê³„
            self.model.eval()
            total_val_loss = 0
            successful_val_batches = 0
            
            print("ğŸ“Š ê²€ì¦ ì¤‘...")
            with torch.no_grad():
                val_progress = tqdm(val_loader, desc=f"ê²€ì¦ ì—í¬í¬ {epoch+1}")
                
                for batch in val_progress:
                    try:
                        input_ids = batch['input_ids'].to(self.device)
                        attention_mask = batch['attention_mask'].to(self.device)
                        token_type_ids = batch['token_type_ids'].to(self.device)
                        
                        outputs = self.model(
                            input_ids=input_ids,
                            attention_mask=attention_mask,
                            token_type_ids=token_type_ids
                        )
                        
                        batch_size_current = input_ids.size(0)
                        targets = torch.ones(batch_size_current, 1).to(self.device)
                        
                        qa_logits = outputs['qa_logits']
                        loss = criterion(qa_logits, targets)
                        
                        total_val_loss += loss.item()
                        successful_val_batches += 1
                        
                        val_progress.set_postfix({'val_loss': f"{loss.item():.4f}"})
                        
                    except Exception as e:
                        print(f"\nâŒ ê²€ì¦ ì˜¤ë¥˜: {e}")
                        continue
            
            avg_val_loss = total_val_loss / successful_val_batches if successful_val_batches > 0 else float('inf')
            val_losses.append(avg_val_loss)
            
            print(f"âœ… ê²€ì¦ ì™„ë£Œ - í‰ê·  ì†ì‹¤: {avg_val_loss:.4f}")
            print_memory_usage()
            
            # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
            if avg_val_loss < best_val_loss:
                best_val_loss = avg_val_loss
                print(f"ğŸ¯ ìµœê³  ì„±ëŠ¥ ê°±ì‹ ! ì²´í¬í¬ì¸íŠ¸ ì €ì¥...")
                self.save_checkpoint(epoch, avg_train_loss, avg_val_loss)
        
        print_step("í›ˆë ¨ ì™„ë£Œ", f"ìµœì¢… í›ˆë ¨ ì†ì‹¤: {train_losses[-1]:.4f}, ìµœì¢… ê²€ì¦ ì†ì‹¤: {val_losses[-1]:.4f}")
        
        return train_losses, val_losses
    
    def save_checkpoint(self, epoch: int, train_loss: float, val_loss: float):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        checkpoint_path = TRAINED_MODEL_DIR / f"checkpoint_epoch_{epoch+1}.pth"
        checkpoint_path.parent.mkdir(parents=True, exist_ok=True)
        
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'train_loss': train_loss,
            'val_loss': val_loss,
            'model_name': self.model_name
        }
        
        torch.save(checkpoint, checkpoint_path)
        print(f"ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {checkpoint_path}")
    
    def save_model(self, save_path):
        """ìµœì¢… ëª¨ë¸ ì €ì¥"""
        print_step("ëª¨ë¸ ì €ì¥", f"ê²½ë¡œ: {save_path}")
        
        try:
            save_path.mkdir(parents=True, exist_ok=True)
            
            # ëª¨ë¸ ê°€ì¤‘ì¹˜ ì €ì¥
            torch.save(self.model.state_dict(), save_path / "pytorch_model.bin")
            
            # ì„¤ì • ì €ì¥
            config = {
                'model_name': self.model_name,
                'hidden_size': self.model.hidden_size,
                'model_type': 'DebugRecipeChatbotModel',
                'training_completed': True,
                'debug_version': True
            }
            
            with open(save_path / "config.json", 'w', encoding='utf-8') as f:
                json.dump(config, f, ensure_ascii=False, indent=2)
            
            # í† í¬ë‚˜ì´ì € ì €ì¥
            tokenizer_path = save_path / "tokenizer"
            tokenizer_path.mkdir(exist_ok=True)
            self.tokenizer.save_pretrained(tokenizer_path)
            
            print("âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ!")
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}")
            raise

def main():
    """ë©”ì¸ í›ˆë ¨ í•¨ìˆ˜ (ë””ë²„ê¹… ê°•í™”)"""
    print("ğŸš€ ë””ë²„ê¹… ê°•í™”ëœ ë ˆì‹œí”¼ ì±—ë´‡ ëª¨ë¸ í›ˆë ¨ ì‹œì‘!")
    print("âš¡ ì§„í–‰ ìƒí™©ì„ ìƒì„¸íˆ ì¶œë ¥í•©ë‹ˆë‹¤...")
    
    print_step("í™˜ê²½ í™•ì¸", "ì‹œìŠ¤í…œ í™˜ê²½ ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ í™•ì¸")
    
    # ë¼ì´ë¸ŒëŸ¬ë¦¬ ë²„ì „ í™•ì¸
    print(f"ğŸ”§ PyTorch ë²„ì „: {torch.__version__}")
    try:
        import transformers
        print(f"ğŸ”§ Transformers ë²„ì „: {transformers.__version__}")
    except:
        print("âŒ Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ")
    
    print_step("ë°ì´í„° í™•ì¸", "QA ë°ì´í„°ì…‹ ì¡´ì¬ í™•ì¸")
    
    if not QA_DATASET_PATH.exists():
        print(f"âŒ QA ë°ì´í„°ì…‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {QA_DATASET_PATH}")
        print("ë¨¼ì € enhanced_qa_generator.pyë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        return
    
    # íŒŒì¼ í¬ê¸° í™•ì¸
    file_size = QA_DATASET_PATH.stat().st_size / (1024 * 1024)
    print(f"âœ… QA íŒŒì¼ í¬ê¸°: {file_size:.1f}MB")
    
    try:
        print_step("í›ˆë ¨ê¸° ì´ˆê¸°í™”", "ë””ë²„ê¹… ê°•í™” í›ˆë ¨ê¸° ìƒì„±")
        trainer = DebugModelTrainer(MODEL_NAME)
        
        print_step("ë°ì´í„° ì¤€ë¹„", "QA ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬")
        trainer.prepare_data(QA_DATASET_PATH)
        
        print_step("ëª¨ë¸ í›ˆë ¨", "ì‹¤ì œ í›ˆë ¨ ì‹œì‘ (ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŒ)")
        
        # ì‘ì€ ì„¤ì •ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ í›ˆë ¨
        train_losses, val_losses = trainer.train(
            num_epochs=1,    # í…ŒìŠ¤íŠ¸ìš© 1 ì—í¬í¬
            batch_size=2,    # ë§¤ìš° ì‘ì€ ë°°ì¹˜
            learning_rate=5e-5
        )
        
        print_step("ëª¨ë¸ ì €ì¥", "í›ˆë ¨ëœ ëª¨ë¸ ì €ì¥")
        trainer.save_model(TRAINED_MODEL_DIR)
        
        print_step("í›ˆë ¨ ì™„ë£Œ", "ëª¨ë“  ë‹¨ê³„ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ")
        print(f"âœ… ìµœì¢… í›ˆë ¨ ì†ì‹¤: {train_losses[-1]:.4f}")
        print(f"âœ… ìµœì¢… ê²€ì¦ ì†ì‹¤: {val_losses[-1]:.4f}")
        
    except Exception as e:
        print(f"âŒ ì¹˜ëª…ì  ì˜¤ë¥˜ ë°œìƒ:")
        print(f"   ì˜¤ë¥˜ ë©”ì‹œì§€: {e}")
        import traceback
        traceback.print_exc()
        
        print(f"\nğŸ’¡ ë¬¸ì œ í•´ê²° ë°©ë²•:")
        print(f"1. ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ: batch_sizeë¥¼ 1ë¡œ ì¤„ì—¬ë³´ì„¸ìš”")
        print(f"2. ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ ì‹œ: ì¸í„°ë„· ì—°ê²°ì„ í™•ì¸í•˜ì„¸ìš”")
        print(f"3. CUDA ì˜¤ë¥˜ ì‹œ: CPU ëª¨ë“œë¡œ ì‹¤í–‰í•´ë³´ì„¸ìš”")
        print(f"4. ê°„ë‹¨í•œ ë²„ì „ ì‹œë„: python model/simple_trainer.py")

if __name__ == "__main__":
    main()