"""
ê°œì„ ëœ ë ˆì‹œí”¼ ì±—ë´‡ ëª¨ë¸ í›ˆë ¨ê¸°
- KcBERT ê¸°ë°˜ QA ëª¨ë¸ ì˜¬ë°”ë¥¸ í›ˆë ¨
- ì§ˆë¬¸-ë‹µë³€ ë§¤ì¹­ í•™ìŠµ
- ì„ë² ë”© ê¸°ë°˜ ìœ ì‚¬ë„ í•™ìŠµ
"""
import json
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from transformers import (
    AutoTokenizer, AutoModel, 
    get_linear_schedule_with_warmup,
    AdamW, AutoConfig
)
from sklearn.model_selection import train_test_split
import numpy as np
from tqdm import tqdm
import sys
import os
from typing import List, Dict, Any, Tuple
import random
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from config import *
class QADataset(Dataset):
    """ì§ˆë¬¸-ë‹µë³€ ë°ì´í„°ì…‹"""
    
    def __init__(self, qa_data: List[Dict[str, Any]], tokenizer, max_length: int = 512):
        self.qa_data = qa_data
        self.tokenizer = tokenizer
        self.max_length = max_length
        
        print(f"ğŸ“Š QA ë°ì´í„°ì…‹ ìƒì„±: {len(qa_data)}ê°œ QA ìŒ")
        
        # ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬
        valid_qa = []
        for qa in qa_data:
            if isinstance(qa, dict) and qa.get('question') and qa.get('answer'):
                valid_qa.append(qa)
        
        self.qa_data = valid_qa
        print(f"âœ… ìœ íš¨í•œ QA: {len(self.qa_data)}ê°œ")
    
    def __len__(self):
        return len(self.qa_data)
    
    def __getitem__(self, idx):
        qa = self.qa_data[idx]
        question = str(qa.get('question', '')).strip()
        answer = str(qa.get('answer', '')).strip()
        
        # ì§ˆë¬¸ê³¼ ë‹µë³€ì„ í•¨ê»˜ ì¸ì½”ë”© (BERT ë°©ì‹)
        encoding = self.tokenizer(
            question,
            answer,
            add_special_tokens=True,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        # ì§ˆë¬¸ë§Œ ì¸ì½”ë”© (ê²€ìƒ‰ìš©)
        question_encoding = self.tokenizer(
            question,
            add_special_tokens=True,
            max_length=256,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'token_type_ids': encoding.get('token_type_ids', torch.zeros_like(encoding['input_ids'])).flatten(),
            'question_input_ids': question_encoding['input_ids'].flatten(),
            'question_attention_mask': question_encoding['attention_mask'].flatten(),
            'question': question,
            'answer': answer
        }

class EnhancedRecipeChatbotModel(nn.Module):
    """í–¥ìƒëœ ë ˆì‹œí”¼ ì±—ë´‡ ëª¨ë¸"""
    
    def __init__(self, model_name: str = "beomi/kcbert-base", hidden_dropout_prob: float = 0.1):
        super(EnhancedRecipeChatbotModel, self).__init__()
        
        print(f"ğŸ”§ ëª¨ë¸ ì´ˆê¸°í™”: {model_name}")
        
        self.config = AutoConfig.from_pretrained(model_name)
        self.config.hidden_dropout_prob = hidden_dropout_prob
        self.config.max_position_embeddings = 512  # â• ì¶”ê°€ ì´ ë¶€ë¶„!

        self.bert = AutoModel.from_pretrained(model_name, config=self.config, ignore_mismatched_sizes=True)
        
        # ëª¨ë¸ ì°¨ì›
        self.hidden_size = self.bert.config.hidden_size
        print(f"ğŸ“ BERT hidden size: {self.hidden_size}")
        
        # QA ë§¤ì¹­ì„ ìœ„í•œ í—¤ë“œ
        self.qa_classifier = nn.Sequential(
            nn.Dropout(hidden_dropout_prob),
            nn.Linear(self.hidden_size, self.hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(hidden_dropout_prob),
            nn.Linear(self.hidden_size // 2, 1)
        )
        
        # ì„ë² ë”© ìƒì„±ì„ ìœ„í•œ í”„ë¡œì ì…˜ í—¤ë“œ
        self.embedding_projection = nn.Sequential(
            nn.Dropout(hidden_dropout_prob),
            nn.Linear(self.hidden_size, self.hidden_size),
            nn.Tanh()
        )
        
        print("âœ… ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def forward(self, input_ids, attention_mask, token_type_ids=None):
        """ìˆœì „íŒŒ"""
        # BERT ì¸ì½”ë”©
        outputs = self.bert(
            input_ids=input_ids,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids,
            return_dict=True
        )
        
        # [CLS] í† í° ì„ë² ë”© ì¶”ì¶œ
        pooled_output = outputs.pooler_output
        
        # QA ë§¤ì¹­ ì ìˆ˜
        qa_logits = self.qa_classifier(pooled_output)
        
        # ì„ë² ë”© ìƒì„±
        embeddings = self.embedding_projection(pooled_output)
        
        return {
            'qa_logits': qa_logits,
            'embeddings': embeddings,
            'pooled_output': pooled_output
        }
    
    def encode_question(self, input_ids, attention_mask):
        """ì§ˆë¬¸ë§Œ ì¸ì½”ë”© (ê²€ìƒ‰ìš©)"""
        with torch.no_grad():
            outputs = self.bert(
                input_ids=input_ids,
                attention_mask=attention_mask,
                return_dict=True
            )
            
            pooled_output = outputs.pooler_output
            embeddings = self.embedding_projection(pooled_output)
            
            return embeddings

class EnhancedModelTrainer:
    """í–¥ìƒëœ ëª¨ë¸ í›ˆë ¨ê¸°"""
    
    def __init__(self, model_name: str = "beomi/kcbert-base"):
        self.model_name = model_name
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ğŸ”§ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {self.device}")
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.unk_token
            print(f"âœ… í† í¬ë‚˜ì´ì € ë¡œë“œ ì„±ê³µ")
        except Exception as e:
            print(f"âŒ í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise
        
        self.model = None
        self.train_dataset = None
        self.val_dataset = None
    
    def load_qa_data(self, qa_dataset_path: str) -> List[Dict[str, Any]]:
        """QA ë°ì´í„° ë¡œë“œ"""
        try:
            with open(qa_dataset_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            print(f"âœ… QA íŒŒì¼ ë¡œë“œ ì„±ê³µ: {qa_dataset_path}")
            
            qa_data = []
            if isinstance(data, dict):
                if 'qa_pairs' in data:
                    qa_data = data['qa_pairs']
                    if 'metadata' in data:
                        print(f"ğŸ“ˆ ë©”íƒ€ë°ì´í„°: {data['metadata']}")
                    if 'statistics' in data:
                        print(f"ğŸ“Š í†µê³„ ì •ë³´ í¬í•¨")
                else:
                    for key, value in data.items():
                        if isinstance(value, list) and len(value) > 0:
                            if isinstance(value[0], dict) and 'question' in value[0]:
                                qa_data = value
                                break
            elif isinstance(data, list):
                qa_data = data
            
            # ìœ íš¨ì„± ê²€ì‚¬
            valid_qa = []
            for qa in qa_data:
                if isinstance(qa, dict) and qa.get('question') and qa.get('answer'):
                    # ë„ˆë¬´ ê¸´ ë‹µë³€ í•„í„°ë§ (í† í° í•œê³„ ê³ ë ¤)
                    question = str(qa['question']).strip()
                    answer = str(qa['answer']).strip()
                    
                    if len(question) >= 3 and len(answer) >= 5 and len(answer) <= 1000:
                        valid_qa.append(qa)
            
            print(f"ğŸ³ ì „ì²´ ìœ íš¨í•œ QA: {len(valid_qa)}ê°œ")
            return valid_qa
            
        except Exception as e:
            print(f"âŒ QA ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return []
    
    def prepare_data(self, qa_dataset_path: str, test_size: float = 0.2):
        """ë°ì´í„° ì¤€ë¹„"""
        print("ğŸš€ QA ë°ì´í„° ì¤€ë¹„ ì¤‘...")
        
        qa_data = self.load_qa_data(qa_dataset_path)
        
        if not qa_data:
            raise ValueError("ìœ íš¨í•œ QA ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # ë°ì´í„° ì…”í”Œ
        random.shuffle(qa_data)
        
        # í›ˆë ¨/ê²€ì¦ ë¶„í• 
        if len(qa_data) >= 10:
            train_data, val_data = train_test_split(
                qa_data, 
                test_size=test_size, 
                random_state=42            )
        else:
            # ì‘ì€ ë°ì´í„°ì…‹ì˜ ê²½ìš°
            split_idx = max(1, int(len(qa_data) * 0.8))
            train_data = qa_data[:split_idx]
            val_data = qa_data[split_idx:] if split_idx < len(qa_data) else qa_data[-1:]
        
        print(f"âœ… í›ˆë ¨ ë°ì´í„°: {len(train_data)}ê°œ")
        print(f"âœ… ê²€ì¦ ë°ì´í„°: {len(val_data)}ê°œ")
        
        # ë°ì´í„°ì…‹ ìƒì„±
        self.train_dataset = QADataset(train_data, self.tokenizer, max_length=512)
        self.val_dataset = QADataset(val_data, self.tokenizer, max_length=512)
        
        return self.train_dataset, self.val_dataset
    
    def initialize_model(self):
        """ëª¨ë¸ ì´ˆê¸°í™”"""
        print("ğŸ”§ ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
        
        try:
            self.model = EnhancedRecipeChatbotModel(self.model_name)
            self.model.to(self.device)
            
            # ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°
            total_params = sum(p.numel() for p in self.model.parameters())
            trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
            
            print(f"âœ… ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
            print(f"   ì´ íŒŒë¼ë¯¸í„°: {total_params:,}")
            print(f"   í›ˆë ¨ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°: {trainable_params:,}")
            
            return self.model
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    def train(self, num_epochs: int = 3, batch_size: int = 8, learning_rate: float = 2e-5):
        """ëª¨ë¸ í›ˆë ¨"""
        if self.model is None:
            self.initialize_model()
        
        if self.train_dataset is None:
            raise ValueError("í›ˆë ¨ ë°ì´í„°ê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # ë°ì´í„°ë¡œë” ìƒì„±
        train_loader = DataLoader(
            self.train_dataset, 
            batch_size=batch_size, 
            shuffle=True,
            num_workers=0,  # Windows í˜¸í™˜ì„±
            pin_memory=True if self.device.type == 'cuda' else False
        )
        
        val_loader = DataLoader(
            self.val_dataset, 
            batch_size=batch_size, 
            shuffle=False,
            num_workers=0,
            pin_memory=True if self.device.type == 'cuda' else False
        )
        
        # ì˜µí‹°ë§ˆì´ì €ì™€ ìŠ¤ì¼€ì¤„ëŸ¬
        optimizer = AdamW(
            self.model.parameters(), 
            lr=learning_rate,
            weight_decay=0.01,
            eps=1e-8
        )
        
        total_steps = len(train_loader) * num_epochs
        scheduler = get_linear_schedule_with_warmup(
            optimizer,
            num_warmup_steps=int(0.1 * total_steps),
            num_training_steps=total_steps
        )
        
        # ì†ì‹¤ í•¨ìˆ˜
        criterion = nn.BCEWithLogitsLoss()
        
        print(f"\nğŸš€ í›ˆë ¨ ì‹œì‘")
        print(f"   ì—í¬í¬: {num_epochs}")
        print(f"   ë°°ì¹˜ í¬ê¸°: {batch_size}")
        print(f"   í•™ìŠµë¥ : {learning_rate}")
        print(f"   ì´ ìŠ¤í…: {total_steps}")
        
        best_val_loss = float('inf')
        train_losses = []
        val_losses = []
        
        for epoch in range(num_epochs):
            print(f"\n{'='*60}")
            print(f"ì—í¬í¬ {epoch + 1}/{num_epochs}")
            print(f"{'='*60}")
            
            # í›ˆë ¨ ë‹¨ê³„
            self.model.train()
            total_train_loss = 0
            
            train_progress = tqdm(train_loader, desc=f"í›ˆë ¨ ì—í¬í¬ {epoch+1}")
            
            for batch_idx, batch in enumerate(train_progress):
                try:
                    optimizer.zero_grad()
                    
                    # ë°ì´í„° GPUë¡œ ì´ë™
                    input_ids = batch['input_ids'].to(self.device)
                    attention_mask = batch['attention_mask'].to(self.device)
                    token_type_ids = batch['token_type_ids'].to(self.device)
                    
                    # ìˆœì „íŒŒ
                    outputs = self.model(
                        input_ids=input_ids,
                        attention_mask=attention_mask,
                        token_type_ids=token_type_ids
                    )
                    
                    # íƒ€ê²Ÿ ìƒì„± (QA ë§¤ì¹­ì€ ëª¨ë‘ positiveë¡œ í•™ìŠµ)
                    batch_size = input_ids.size(0)
                    targets = torch.ones(batch_size, 1).to(self.device)
                    
                    # ì†ì‹¤ ê³„ì‚°
                    qa_logits = outputs['qa_logits']
                    loss = criterion(qa_logits, targets)
                    
                    # ì—­ì „íŒŒ
                    loss.backward()
                    
                    # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                    
                    optimizer.step()
                    scheduler.step()
                    
                    total_train_loss += loss.item()
                    
                    train_progress.set_postfix({
                        'loss': f"{loss.item():.4f}",
                        'lr': f"{scheduler.get_last_lr()[0]:.2e}"
                    })
                    
                except Exception as e:
                    print(f"\nâŒ ë°°ì¹˜ {batch_idx} í›ˆë ¨ ì˜¤ë¥˜: {e}")
                    continue
            
            avg_train_loss = total_train_loss / len(train_loader)
            train_losses.append(avg_train_loss)
            
            # ê²€ì¦ ë‹¨ê³„
            self.model.eval()
            total_val_loss = 0
            
            with torch.no_grad():
                val_progress = tqdm(val_loader, desc=f"ê²€ì¦ ì—í¬í¬ {epoch+1}")
                
                for batch in val_progress:
                    try:
                        input_ids = batch['input_ids'].to(self.device)
                        attention_mask = batch['attention_mask'].to(self.device)
                        token_type_ids = batch['token_type_ids'].to(self.device)
                        
                        outputs = self.model(
                            input_ids=input_ids,
                            attention_mask=attention_mask,
                            token_type_ids=token_type_ids
                        )
                        
                        batch_size = input_ids.size(0)
                        targets = torch.ones(batch_size, 1).to(self.device)
                        
                        qa_logits = outputs['qa_logits']
                        loss = criterion(qa_logits, targets)
                        
                        total_val_loss += loss.item()
                        
                        val_progress.set_postfix({'val_loss': f"{loss.item():.4f}"})
                        
                    except Exception as e:
                        print(f"\nâŒ ê²€ì¦ ì˜¤ë¥˜: {e}")
                        continue
            
            avg_val_loss = total_val_loss / len(val_loader) if len(val_loader) > 0 else float('inf')
            val_losses.append(avg_val_loss)
            
            # ì—í¬í¬ ê²°ê³¼ ì¶œë ¥
            print(f"\nğŸ“Š ì—í¬í¬ {epoch + 1} ê²°ê³¼:")
            print(f"   í›ˆë ¨ ì†ì‹¤: {avg_train_loss:.4f}")
            print(f"   ê²€ì¦ ì†ì‹¤: {avg_val_loss:.4f}")
            
            # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
            if avg_val_loss < best_val_loss:
                best_val_loss = avg_val_loss
                print(f"   ğŸ¯ ìµœê³  ì„±ëŠ¥ ê°±ì‹ ! ëª¨ë¸ ì €ì¥...")
                self.save_checkpoint(epoch, avg_train_loss, avg_val_loss)
        
        print(f"\nğŸ‰ í›ˆë ¨ ì™„ë£Œ!")
        print(f"   ìµœì¢… í›ˆë ¨ ì†ì‹¤: {train_losses[-1]:.4f}")
        print(f"   ìµœì¢… ê²€ì¦ ì†ì‹¤: {val_losses[-1]:.4f}")
        print(f"   ìµœê³  ê²€ì¦ ì†ì‹¤: {best_val_loss:.4f}")
        
        return train_losses, val_losses
    
    def save_checkpoint(self, epoch: int, train_loss: float, val_loss: float):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        checkpoint_path = TRAINED_MODEL_DIR / f"checkpoint_epoch_{epoch+1}.pth"
        checkpoint_path.parent.mkdir(parents=True, exist_ok=True)
        
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'train_loss': train_loss,
            'val_loss': val_loss,
            'model_name': self.model_name
        }
        
        torch.save(checkpoint, checkpoint_path)
    
    def save_model(self, save_path):
        """ìµœì¢… ëª¨ë¸ ì €ì¥"""
        print(f"ğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘: {save_path}")
        
        try:
            save_path.mkdir(parents=True, exist_ok=True)
            
            # ëª¨ë¸ ê°€ì¤‘ì¹˜ ì €ì¥
            torch.save(self.model.state_dict(), save_path / "pytorch_model.bin")
            
            # ì„¤ì • ì €ì¥
            config = {
                'model_name': self.model_name,
                'hidden_size': self.model.hidden_size,
                'model_type': 'EnhancedRecipeChatbotModel',
                'training_completed': True
            }
            
            with open(save_path / "config.json", 'w', encoding='utf-8') as f:
                json.dump(config, f, ensure_ascii=False, indent=2)
            
            # í† í¬ë‚˜ì´ì € ì €ì¥
            tokenizer_path = save_path / "tokenizer"
            tokenizer_path.mkdir(exist_ok=True)
            self.tokenizer.save_pretrained(tokenizer_path)
            
            print("âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ!")
            print(f"   ëª¨ë¸ ê°€ì¤‘ì¹˜: pytorch_model.bin")
            print(f"   ì„¤ì • íŒŒì¼: config.json")
            print(f"   í† í¬ë‚˜ì´ì €: tokenizer/")
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}")
            raise

def main():
    """ë©”ì¸ í›ˆë ¨ í•¨ìˆ˜"""
    print("ğŸš€ ê°œì„ ëœ ë ˆì‹œí”¼ ì±—ë´‡ ëª¨ë¸ í›ˆë ¨ ì‹œì‘!")
    
    if not QA_DATASET_PATH.exists():
        print(f"âŒ QA ë°ì´í„°ì…‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {QA_DATASET_PATH}")
        print("ë¨¼ì € enhanced_qa_generator.pyë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        return
    
    try:
        # í›ˆë ¨ê¸° ì´ˆê¸°í™”
        trainer = EnhancedModelTrainer(MODEL_NAME)
        
        # ë°ì´í„° ì¤€ë¹„
        trainer.prepare_data(QA_DATASET_PATH)
        
        # ëª¨ë¸ í›ˆë ¨
        train_losses, val_losses = trainer.train(
            num_epochs=NUM_EPOCHS,
            batch_size=BATCH_SIZE,
            learning_rate=LEARNING_RATE
        )
        
        # ëª¨ë¸ ì €ì¥
        trainer.save_model(TRAINED_MODEL_DIR)
        
        print(f"\nğŸ‰ í›ˆë ¨ ì™„ë£Œ!")
        print(f"âœ… ìµœì¢… í›ˆë ¨ ì†ì‹¤: {train_losses[-1]:.4f}")
        print(f"âœ… ìµœì¢… ê²€ì¦ ì†ì‹¤: {val_losses[-1]:.4f}")
        
    except Exception as e:
        print(f"âŒ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    from transformers import AutoConfig
    config = AutoConfig.from_pretrained("beomi/kcbert-base")
    print(config.max_position_embeddings)  # ë§Œì•½ 300ì´ë©´ 512ë¡œ ë³€ê²½


    main()
